{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: on this notebook I am just practicing concepts of multiple linear regression. I am not considering some aspects related to machine learning like the imputation of missing values or the normalisation of the predictor variables. \n",
    "\n",
    "## Loading and processing the data\n",
    "\n",
    "The dataset used in this notebook is the Auto MPG dataset from the UCI Machine Learning Repository: (https://archive.ics.uci.edu/ml/datasets/Auto+MPG).\n",
    "\n",
    "The mpg column corresponds to the response variable. All the predictor variables can be interpreted as numerical variables except origin, that is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/auto-mpg.csv', sep='\\s+', header=None, usecols=range(8))\n",
    "df.columns = ['mpg', 'cylinders', 'displacement',\n",
    "              'horsepower', 'weight', 'acceleration',\n",
    "              'model_year', 'origin']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing rows with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[~(df.horsepower == '?')]\n",
    "df.horsepower = df.horsepower.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,4)\n",
    "\n",
    "ax[0,0].hist(df.mpg)\n",
    "ax[0,0].set_title('mpg distribution')\n",
    "\n",
    "ax[0,1].hist(df.cylinders)\n",
    "ax[0,1].set_title('cylinders distribution')\n",
    "\n",
    "ax[0,2].hist(df.displacement)\n",
    "ax[0,2].set_title('displacement distribution')\n",
    "\n",
    "ax[0,3].hist(df.horsepower)\n",
    "ax[0,3].set_title('horsepower distribution')\n",
    "\n",
    "ax[1,0].hist(df.weight)\n",
    "ax[1,0].set_title('weight distribution')\n",
    "\n",
    "ax[1,1].hist(df.acceleration)\n",
    "ax[1,1].set_title('acceleration distribution')\n",
    "\n",
    "ax[1,2].hist(df.model_year)\n",
    "ax[1,2].set_title('model_year distribution')\n",
    "\n",
    "ax[1,3].hist(df.origin)\n",
    "ax[1,3].set_title('origin distribution')\n",
    "\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(4)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are testing now whether any pair of predictors are collinear, that is, whether any pair of predictors is correlated. Collinearity is an issue since small variations in the data or the model may produce large erratic changes in the point estimates of the multiple regression model. This does not affect the accuracy of the prediction as a whole, but may have an impact on the accuracy of the prediction of each individual predictor.\n",
    "\n",
    "There are 7 predictor variables, making a total of 7*6/2 = 21 combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,5)\n",
    "row = 0\n",
    "col = 0\n",
    "\n",
    "for i in range(1,len(df.columns)):\n",
    "    for j in range(i+1,len(df.columns)):\n",
    "        \n",
    "        var1 = df[df.columns[i]]\n",
    "        var2 = df[df.columns[j]]\n",
    "        \n",
    "        ax[row, col].scatter(var1, var2)\n",
    "        \n",
    "        R = 1/(len(df)-1)*np.sum(((var1-var1.mean())/var1.std())*((var2-var2.mean())/var2.std()))\n",
    "        \n",
    "        ax[row, col].set_title(df.columns[i] + ' vs ' + df.columns[j] + '\\nR = ' + str(R))\n",
    "        \n",
    "        col = col + 1\n",
    "        if col > 4:\n",
    "            col = 0\n",
    "            row = row + 1\n",
    "            \n",
    "fig.set_figwidth(14)\n",
    "fig.set_figheight(14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a somehow strong linear relationship between a set of four variables: displacement, horsepower, weight and cylinders. We will examine later, during model selection, how this may affect the accuracy of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use categorical predictor variables in our multiple regression model (in our case only the 'origin' predictor is categorical) we need to transform them into indicator or dummy variables. An indicator variable takes the value 1 or 0 depending on whether the value represented by the indicator variable was the value of the original categorical variable in the original dataset. If the categorical variable only has two levels only one indicator variable is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(df.origin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in [1,2]:\n",
    "    df['origin_' + str(v)] = (df.origin == v).astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the multiple regression model\n",
    "\n",
    "We will use least squares to fit the multiple regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "        'acceleration', 'model_year', 'origin_1', 'origin_2']\n",
    "\n",
    "Y = df.mpg.values\n",
    "X = df[columns].values\n",
    "\n",
    "X = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "\n",
    "B = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), Y)\n",
    "\n",
    "print('Fitted linear model:')\n",
    "print('intercept: ' + str(B[0]))\n",
    "for i in range(len(columns)):\n",
    "    print(columns[i] + ': ' + str(B[i+1]))\n",
    "    \n",
    "residuals = Y - np.dot(X, B)\n",
    "\n",
    "adj_r2 = 1 - ((Y.shape[0]-1)*(np.std(residuals)**2)/((Y.shape[0]-len(columns)-1)*(np.std(Y)**2)))\n",
    "print('\\nAdjusted R2: ' + str(adj_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's use an added variable plots to visualise the relationship between the response variable and each predictor variable, individually. The added variable plot for x_0 is built by plotting the residuals of the response variable after fitting the model with all the predictor variables except x_0 vs. the residuals of x_0 after fitting a regression for x_0 with all the other predictor variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def added_variable_plot(df, predictor, ax):\n",
    "    Y = df.mpg.values\n",
    "    X_0 = df[predictor].values\n",
    "    \n",
    "    X = df[columns]\n",
    "    del X[predictor]\n",
    "    X = X.values\n",
    "    \n",
    "    X = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "    \n",
    "    B_y = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), Y)\n",
    "    B_x0 = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), X_0)\n",
    "    \n",
    "    residuals_y = Y - np.dot(X, B_y)\n",
    "    residuals_x0 = X_0 - np.dot(X, B_x0)\n",
    "    \n",
    "    ax.scatter(residuals_x0, residuals_y)\n",
    "    ax.set_xlabel(predictor)\n",
    "    ax.set_ylabel('mpg')\n",
    "    \n",
    "    # Least squares of the residuals(\n",
    "    R = 1/(Y.shape[0] - 1)*np.sum((residuals_x0 - np.mean(residuals_x0))/np.std(residuals_x0)*(residuals_y - np.mean(residuals_y))/np.std(residuals_y))\n",
    "    b1 = R*np.std(residuals_y)/np.std(residuals_x0)\n",
    "    b0 = np.mean(residuals_y) - b1*np.mean(residuals_x0)\n",
    "    xo = np.min(residuals_x0) - 0.1\n",
    "    xf = np.max(residuals_x0) + 0.1\n",
    "    ax.plot([xo, xf], [b1*xo + b0, b1*xf + b0], 'r', lw=2)\n",
    "    ax.set_title('R = ' + str(R))\n",
    "    \n",
    "fig, ax = plt.subplots(3,3)\n",
    "row = 0\n",
    "column = 0\n",
    "for c in columns:\n",
    "    added_variable_plot(df, c, ax[row][column])\n",
    "    column = column + 1\n",
    "    if column > 2:\n",
    "        column = 0\n",
    "        row = row + 1\n",
    "\n",
    "fig.set_figwidth(8)\n",
    "fig.set_figheight(8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking model assumptions\n",
    "\n",
    "Is a linear multiple regression model appropriate with this data? The conditions we need to evaluate are:\n",
    "\n",
    "- The residuals are normally distributed\n",
    "- The variability of the residuals is nearly normal\n",
    "- The residuals are independent\n",
    "- Each variable is linearly related to the outcome\n",
    "\n",
    "We will assess these conditions by means of a series of plots. We first build a q-q plot to test whether the residuals are normally distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = np.arange(0.01,0.99,0.01)\n",
    "q_theoretical = [st.norm.ppf(i, loc=np.mean(residuals), scale=np.std(residuals)) for i in quantiles]\n",
    "\n",
    "q_sample = [np.percentile(residuals, i*100) for i in quantiles]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(q_sample, q_theoretical, color='blue')\n",
    "\n",
    "min_value = min(np.min(q_theoretical), np.min(q_sample))\n",
    "max_value = max(np.max(q_theoretical), np.max(q_sample))\n",
    "ax.plot([min_value, max_value], [min_value, max_value], 'k--')\n",
    "\n",
    "ax.set_xlabel('residuals')\n",
    "ax.set_ylabel('theoretical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals seem to be normally distributed with maybe some outliers on the right side of the distribution.\n",
    "\n",
    "Let's test now whether the variability of the residuals is nearly constant, by means of a plot of the residuals against the fitted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(np.dot(X, B), residuals)\n",
    "ax.set_xlabel('fitted values')\n",
    "ax.set_ylabel('residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be non-linearity in the data. A linear multiple regression model may not be the best model for this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the independence of the residuals we plot the residuals in the order in which the observations were observed. We are assumed that the data in the original dataset was stored in the order in which the observations were made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(residuals, 'o')\n",
    "ax.set_xlabel('order of collection')\n",
    "ax.set_ylabel('residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a non-linear pattern in this plot as well. The variability seems to be higher for some subsets of observations that are very close to each other in time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test the residuals against each predictor variable, in order to determine whether there is any different variability between groups in the case of the categorical variable and whether the variability is constant in the case of the numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = ['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "        'acceleration', 'model_year', 'origin']\n",
    "\n",
    "fig, ax = plt.subplots(2,4)\n",
    "row = 0\n",
    "col = 0\n",
    "for c in original_columns:\n",
    "    ax[row, col].plot(df[c], residuals, 'o')\n",
    "    ax[row, col].set_xlabel(c)\n",
    "    ax[row, col].set_ylabel('residuals')\n",
    "    \n",
    "    col = col + 1\n",
    "    if col > 3:\n",
    "        col = 0\n",
    "        row = row + 1\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(6)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plots are also showing the non-linear relationship between some variables and the residuals, as well as different variability between groups in the case of the cylinders variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of these plots is to determine whether some structure still exists in the data after fitting the multiple regression model. We may need to adjust the model to try to account for the extra structure (for instance, the non-constant variability of the residuals). If we are not able to do so, we can still report the results, that is, the fitted linear model as long as we also report it shortcomings. \n",
    "\n",
    "In the case of the specific dataset in this notebook, it seems that a linear model may not be the best type of model to predict mpg from the rest of the variables. There's a clear non-linear trend in the data, and other assumptions required to apply a multiple regression linear model do not hold as well.\n",
    "\n",
    "However, and for the purpose of finish demonstrating some aspects of multiple linear regression (model selection) we will keep using this dataset as an example in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model selection\n",
    "\n",
    "The full model, that is, the model which includes the complete set of predictor variables, may not be the best model in terms of prediction accuracy. Model selection is a collection of techniques which aim is the selection of a subset of predictor variables in order to increase the prediction accuracy of the model.\n",
    "\n",
    "In this section we will demonstrate how to use stepwise model selection. In stepwise model selection we proceed by adding (forward selection) or removing (backward elimination) one predictor variable at a time. To evaluate the model produced at each step we can either use adjusted R^2 or the p-value of the estimate of the slope for each variable. We will use adjusted R^2 in these examples since it produces models with higher prediction accuracy in general.\n",
    "\n",
    "**Forward selection**. We start off with an empty model. At each iteration we add the predictor variable that produces the highest increase in the value of adjusted R^2. We keep repeating this step until we cannot improve the value of adjusted R^2 any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y = df.mpg.values\n",
    "\n",
    "pending = [c for c in columns]\n",
    "model = []\n",
    "previous_adj_r2 = -1000\n",
    "end = False\n",
    "it = 1\n",
    "while not end:\n",
    "    print('Iteration ' + str(it))#\n",
    "    end = True\n",
    "    for p in pending:\n",
    "        # Add the variable to the model\n",
    "        model.append(p)\n",
    "        # Fit the model\n",
    "        X = df[model].values\n",
    "        X = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "        if np.linalg.det(np.dot(X.T, X)) == 0:\n",
    "            print('   ' + p + ': dependent variable')\n",
    "        else:\n",
    "            B = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), Y)\n",
    "            # Calculate the adjusted R^2\n",
    "            residuals = Y - np.dot(X, B)\n",
    "            adj_r2 = 1 - ((Y.shape[0]-1)*(np.std(residuals)**2)/((Y.shape[0]-len(columns)-1)*(np.std(Y)**2)))\n",
    "            # Restore the model\n",
    "            model.remove(p)\n",
    "\n",
    "            print('   ' + p + ': ' + str(adj_r2))\n",
    "            if adj_r2 > previous_adj_r2:\n",
    "                previous_adj_r2 = adj_r2\n",
    "                end = False\n",
    "                to_add = p\n",
    "    if not end:\n",
    "        print('\\nAdding ' + to_add)\n",
    "        model.append(to_add)\n",
    "    print('------------------')\n",
    "    it = it + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward elimination**. We start off from the full model. At each iteration we add the predictor variable that produces the highest increase in the value of adjusted R^2. We keep repeating this step until we cannot improve the value of adjusted R^2 any further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The ending result** may differ after applying forward selection and backward elimination. If that is the case, we keep the model with the highest adjusted R^2 value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
