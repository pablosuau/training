{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: on this notebook I am just practicing concepts of multiple linear regression. I am not considering some aspects related to machine learning like the imputation of missing values or the normalisation of the predictor variables. \n",
    "\n",
    "## Loading and processing the data\n",
    "\n",
    "The dataset used in this notebook is the Auto MPG dataset from the UCI Machine Learning Repository: (https://archive.ics.uci.edu/ml/datasets/Auto+MPG).\n",
    "\n",
    "The mpg column corresponds to the response variable. All the predictor variables can be interpreted as numerical variables except origin, that is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/auto-mpg.csv', sep='\\s+', header=None, usecols=range(8))\n",
    "df.columns = ['mpg', 'cylinders', 'displacement',\n",
    "              'horsepower', 'weight', 'acceleration',\n",
    "              'model_year', 'origin']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing rows with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[~(df.horsepower == '?')]\n",
    "df.horsepower = df.horsepower.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,4)\n",
    "\n",
    "ax[0,0].hist(df.mpg)\n",
    "ax[0,0].set_title('mpg distribution')\n",
    "\n",
    "ax[0,1].hist(df.cylinders)\n",
    "ax[0,1].set_title('cylinders distribution')\n",
    "\n",
    "ax[0,2].hist(df.displacement)\n",
    "ax[0,2].set_title('displacement distribution')\n",
    "\n",
    "ax[0,3].hist(df.horsepower)\n",
    "ax[0,3].set_title('horsepower distribution')\n",
    "\n",
    "ax[1,0].hist(df.weight)\n",
    "ax[1,0].set_title('weight distribution')\n",
    "\n",
    "ax[1,1].hist(df.acceleration)\n",
    "ax[1,1].set_title('acceleration distribution')\n",
    "\n",
    "ax[1,2].hist(df.model_year)\n",
    "ax[1,2].set_title('model_year distribution')\n",
    "\n",
    "ax[1,3].hist(df.origin)\n",
    "ax[1,3].set_title('origin distribution')\n",
    "\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(4)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are testing now whether any pair of predictors are collinear, that is, whether any pair of predictors is correlated. Collinearity is an issue since small variations in the data or the model may produce large erratic changes in the point estimates of the multiple regression model. This does not affect the accuracy of the prediction as a whole, but may have an impact on the accuracy of the prediction of each individual predictor.\n",
    "\n",
    "There are 7 predictor variables, making a total of 7*6/2 = 21 combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,5)\n",
    "row = 0\n",
    "col = 0\n",
    "\n",
    "for i in range(1,len(df.columns)):\n",
    "    for j in range(i+1,len(df.columns)):\n",
    "        \n",
    "        var1 = df[df.columns[i]]\n",
    "        var2 = df[df.columns[j]]\n",
    "        \n",
    "        ax[row, col].scatter(var1, var2)\n",
    "        \n",
    "        R = 1/(len(df)-1)*np.sum(((var1-var1.mean())/var1.std())*((var2-var2.mean())/var2.std()))\n",
    "        \n",
    "        ax[row, col].set_title(df.columns[i] + ' vs ' + df.columns[j] + '\\nR = ' + str(R))\n",
    "        \n",
    "        col = col + 1\n",
    "        if col > 4:\n",
    "            col = 0\n",
    "            row = row + 1\n",
    "            \n",
    "fig.set_figwidth(14)\n",
    "fig.set_figheight(14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a somehow strong linear relationship between a set of four variables: displacement, horsepower, weight and cylinders. We will examine later, during model selection, how this may affect the accuracy of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use categorical predictor variables in our multiple regression model (in our case only the 'origin' predictor is categorical) we need to transform them into indicator or dummy variables. An indicator variable takes the value 1 or 0 depending on whether the value represented by the indicator variable was the value of the original categorical variable in the original dataset. If the categorical variable only has two levels only one indicator variable is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(df.origin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in [1,2]:\n",
    "    df['origin_' + str(v)] = (df.origin == v).astype(int)\n",
    "del df['origin']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the multiple regression model\n",
    "\n",
    "We will use least squares to fit the multiple regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "        'acceleration', 'model_year', 'origin_1', 'origin_2']\n",
    "\n",
    "Y = df.mpg.values\n",
    "X = df[columns].values\n",
    "\n",
    "X = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "\n",
    "B = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), Y)\n",
    "\n",
    "print('Fitted linear model:')\n",
    "print('intercept: ' + str(B[0]))\n",
    "for i in range(len(columns)):\n",
    "    print(columns[i] + ': ' + str(B[i+1]))\n",
    "    \n",
    "residuals = Y - np.dot(X, B)\n",
    "\n",
    "adj_r2 = 1 - ((Y.shape[0]-1)*(np.std(residuals)**2)/((Y.shape[0]-len(columns)-1)*(np.std(Y)**2)))\n",
    "print('\\nAdjusted R2: ' + str(adj_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's use an added variable plots to visualise the relationship between the response variable and each predictor variable, individually. The added variable plot for x_0 is built by plotting the residuals of the response variable after fitting the model with all the predictor variables except x_0 vs. the residuals of x_0 after fitting a regression for x_0 with all the other predictor variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def added_variable_plot(df, predictor, ax):\n",
    "    Y = df.mpg.values\n",
    "    X_0 = df[predictor].values\n",
    "    \n",
    "    X = df[columns]\n",
    "    del X[predictor]\n",
    "    X = X.values\n",
    "    \n",
    "    X = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "    \n",
    "    B_y = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), Y)\n",
    "    B_x0 = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), X_0)\n",
    "    \n",
    "    residuals_y = Y - np.dot(X, B_y)\n",
    "    residuals_x0 = X_0 - np.dot(X, B_x0)\n",
    "    \n",
    "    ax.scatter(residuals_x0, residuals_y)\n",
    "    ax.set_xlabel(predictor)\n",
    "    ax.set_ylabel('mpg')\n",
    "    \n",
    "    # Least squares of the residuals(\n",
    "    R = 1/(Y.shape[0] - 1)*np.sum((residuals_x0 - np.mean(residuals_x0))/np.std(residuals_x0)*(residuals_y - np.mean(residuals_y))/np.std(residuals_y))\n",
    "    b1 = R*np.std(residuals_y)/np.std(residuals_x0)\n",
    "    b0 = np.mean(residuals_y) - b1*np.mean(residuals_x0)\n",
    "    xo = np.min(residuals_x0) - 0.1\n",
    "    xf = np.max(residuals_x0) + 0.1\n",
    "    ax.plot([xo, xf], [b1*xo + b0, b1*xf + b0], 'r', lw=2)\n",
    "    ax.set_title('R = ' + str(R))\n",
    "    \n",
    "fig, ax = plt.subplots(3,3)\n",
    "row = 0\n",
    "column = 0\n",
    "for c in columns:\n",
    "    added_variable_plot(df, c, ax[row][column])\n",
    "    column = column + 1\n",
    "    if column > 2:\n",
    "        column = 0\n",
    "        row = row + 1\n",
    "\n",
    "fig.set_figwidth(8)\n",
    "fig.set_figheight(8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking model assumptions\n",
    "\n",
    "Is a linear multiple regression model appropriate with this data? The conditions we need to evaluate are:\n",
    "\n",
    "- The residuals are normally distributed\n",
    "- The variability of the residuals is nearly normal\n",
    "- The residuals are independent\n",
    "- Each variable is linearly related to the outcome\n",
    "\n",
    "We will assess these conditions by means of a series of plots. We first build a q-q plot to test whether the residuals are normally distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = np.arange(0.01,0.99,0.01)\n",
    "q_theoretical = [st.norm.ppf(i, loc=np.mean(residuals), scale=np.std(residuals)) for i in quantiles]\n",
    "\n",
    "q_sample = [np.percentile(residuals, i*100) for i in quantiles]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(q_sample, q_theoretical, color='blue')\n",
    "\n",
    "min_value = min(np.min(q_theoretical), np.min(q_sample))\n",
    "max_value = max(np.max(q_theoretical), np.max(q_sample))\n",
    "ax.plot([min_value, max_value], [min_value, max_value], 'k--')\n",
    "\n",
    "ax.set_xlabel('residuals')\n",
    "ax.set_ylabel('theoretical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals seem to be normally distributed with maybe some outliers on the right side of the distribution.\n",
    "\n",
    "Let's test now whether the variability of the residuals is nearly constant, by means of a plot of the residuals against the fitted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(np.dot(X, B), residuals)\n",
    "ax.set_xlabel('fitted values')\n",
    "ax.set_ylabel('residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be non-linearity in the data. A linear multiple regression model may not be the best model for this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of these plots is to determine whether some structure still exists in the data after fitting the multiple regression model. We may need to adjust the model to try to account for the extra structure (for instance, the non-constant variability of the residuals). If we are not able to do so, we can still report the results, that is, the fitted linear model as long as we also report it shortcomings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
