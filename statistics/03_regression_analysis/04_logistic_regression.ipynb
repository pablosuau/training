{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: on this notebook I am just practicing concepts of logistic regression. I am not considering some aspects related to machine learning like the imputation of missing values or the normalisation of the predictor variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and processing the data\n",
    "\n",
    "The dataset used in this notebook is an example dataset about accessing graduate school which was obtained from https://stats.idre.ucla.edu/r/dae/logit-regression/. The `admit` column corresponds to the two-level categorical response variable. The variables containing the `gre` and `gpa` scores of the candidate are numerical, whereas the variable `rank`, that indicates the prestige of the school, is categorical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/binary.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the distribution of the predictor variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3)\n",
    "\n",
    "ax[0].hist(df['gre'])\n",
    "ax[0].set_title('gre')\n",
    "\n",
    "ax[1].hist(df['gpa'])\n",
    "ax[1].set_title('gpa')\n",
    "\n",
    "ax[2].hist(df['rank'])\n",
    "ax[2].set_title('rank')\n",
    "\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there collinearity between any pair of variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_R(df, col1, col2):\n",
    "    var1 = df[col1]\n",
    "    var2 = df[col2]    \n",
    "    return 1/(len(df)-1)*np.sum(((var1-var1.mean())/var1.std())*((var2-var2.mean())/var2.std()))\n",
    "\n",
    "fig, ax = plt.subplots(1,3)\n",
    "\n",
    "ax[0].plot(df['gre'], df['gpa'], 'o')\n",
    "ax[0].set_title('gre vs gpa\\nR = ' + str(compute_R(df, 'gre', 'gpa')))\n",
    "\n",
    "ax[1].plot(df['gre'], df['rank'], 'o')\n",
    "ax[1].set_title('gre vs rank\\nR = ' + str(compute_R(df, 'gre', 'rank')))\n",
    "\n",
    "ax[2].plot(df['gpa'], df['rank'], 'o')\n",
    "ax[2].set_title('gpa vs rank\\nR = ' + str(compute_R(df, 'gpa', 'rank')))\n",
    "\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building indicator variables to replace the rank categorical variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.unique(df['rank'])[0:-1]\n",
    "for v in values:\n",
    "    df['rank_' + str(v)] = (df['rank'] == v).astype(int)\n",
    "del df['rank']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression is a type of generalised linear model in which the response variable is a two-level categorical variable that, for each observation, takes the value Yi = 1 with probability pi and the value Yi = 0 with probability Yi = 0.\n",
    "\n",
    "A generalised linear model is a generalisation of linear regression in which the residuals can be non-normally distributed. This is achieved by linking the response variable to a multiple regression model by means of a transformation variable, usually the logit function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "p = np.arange(0.01, 0.99, 0.01)\n",
    "logit = np.log(p/(1-p))\n",
    "ax.plot(p, logit)\n",
    "ax.set_xlabel('pi')\n",
    "ax.set_ylabel('logit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model has the following form:\n",
    "```\n",
    "logit(pi) = b0 + b1*x1i + b2*x2i + ... + bk*xki\n",
    "```\n",
    "In order to fit a logistic regression model a function based on Newton method for numerical optimisation is commonly used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['admit']\n",
    "# Intercept is not included by default\n",
    "X = df[['gre', 'gpa', 'rank_1', 'rank_2', 'rank_3']]\n",
    "X = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "   \n",
    "\n",
    "logit_model = sm.Logit(Y, X)\n",
    "result = logit_model.fit()\n",
    "# The following line is a workaround to make summary work\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we plot the predicted probabilities of the observations versus the value returned by the logistic regression model for these observations, we can see the shape of the logit function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict returns the probabilities\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(result.predict(X), result.fittedvalues, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the residuals as the difference between the value of the response variable and the probability returned by the logistic regression model. This plot demonstrates the independence of the observations, since we cannot see any pattern in the data. The residuals are split into two groups due to the fact that the reponse variable is a two level categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(df['admit'] - result.predict(X), 'o')\n",
    "ax.set_xlabel('observation')\n",
    "ax.set_ylabel('residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another condition to apply logistic regression, aside from the observations being independent of each other, is that there exists a linear relationship between logit(pi) and each predictor variable, when the rest of the predictor variables are held constant. We can test this condition by plotting residuals versus the values of each predictor variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,len(predictors))\n",
    "\n",
    "residuals = df['admit'] - result.predict(X)\n",
    "predictors = ['gre', 'gpa', 'rank_1', 'rank_2', 'rank_3']\n",
    "\n",
    "for i in range(len(predictors)):\n",
    "    ax[i].plot(df[predictors[i]], residuals, 'o')\n",
    "    ax[i].set_xlabel(predictors[i])\n",
    "    ax[i].set_ylabel('residuals')\n",
    "    \n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(3)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linearity seems to be fine in most cases, except maybe in the case of variables rank-2 and rank_3, that have different variabilities between groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.params.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.fittedvalues # Logit value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the probability values from the logit predictions of the model for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
