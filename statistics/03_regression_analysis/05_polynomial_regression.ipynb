{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I practice curve fitting by means of polynomial regression. I will apply a least squares approach, similar to the one I applied for multiple linear regression in notebook 3. \n",
    "\n",
    "## Generate the data\n",
    "\n",
    "I am generate a synthetic dataset by using a cubic curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(range(1,11)).reshape(10, 1)\n",
    "X = np.concatenate((np.ones((10, 1)), x, x**2, x**3), axis = 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am adding some random noise after calculating the value of the dependent variable. The magnitude of the random noise is high, so it can be clearly visible on the plots. Unfortunately, this means that the predicted coefficients of the model will be quite different to the ones used to calculate the value of the dependent variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.array([5000, 2.3, 1.3, 10]).reshape(1, 4)\n",
    "y = np.dot(beta, X.T).T\n",
    "# Adding noise\n",
    "y = y + 1000*np.random.random((10, 1))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x.flatten(), y.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a linear model\n",
    "\n",
    "Using least squares to fit a linear model for the data. The adjusted R2 value is not too bad, but the residuals plot shows a clear patter after fitting the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lin = np.concatenate((np.ones((10, 1)), x), axis = 1)\n",
    "B = np.dot(np.dot(np.linalg.inv(np.dot(X_lin.T, X_lin)), X_lin.T), y)\n",
    "\n",
    "print('Fitted linear model:')\n",
    "print(B)\n",
    "    \n",
    "residuals = y - np.dot(X_lin, B)\n",
    "adj_r2 = 1 - ((y.shape[0] - 1) * (np.std(residuals) ** 2) / ((y.shape[0] - 2 - 1) * (np.std(y) ** 2)))\n",
    "print('\\nAdjusted R2: ' + str(adj_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "y_pred_lin = np.dot(B.T, X_lin.T).T\n",
    "ax[0].scatter(x.flatten(), y.flatten())\n",
    "ax[0].plot(range(1, 11), y_pred_lin, 'red')\n",
    "ax[0].set_title('fitted model')\n",
    "\n",
    "ax[1].scatter(x.flatten(), residuals)\n",
    "ax[1].set_title('residual plot - linear')\n",
    "\n",
    "fig.set_figwidth(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a polynomial model\n",
    "\n",
    "In this case we fit a cubic model. The residuals in this case seem to be normally distributed, clearly indicating that there is no clear pattern in the residuals after fitting the polynomial model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "\n",
    "print('Fitted polynomial model:')\n",
    "print(B)\n",
    "    \n",
    "residuals = y - np.dot(X, B)\n",
    "adj_r2 = 1 - ((y.shape[0] - 1) * (np.std(residuals) ** 2) / ((y.shape[0] - 2 - 1) * (np.std(y) ** 2)))\n",
    "print('\\nAdjusted R2: ' + str(adj_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "y_pred = np.dot(B.T, X.T).T\n",
    "ax[0].scatter(x.flatten(), y.flatten())\n",
    "ax[0].plot(range(1, 11), y_pred_lin, 'red')\n",
    "ax[0].plot(range(1, 11), y_pred,'green')\n",
    "ax[0].set_title('fitted models')\n",
    "\n",
    "ax[1].scatter(x.flatten(), residuals)\n",
    "ax[1].set_title('residual plot - polynomial')\n",
    "\n",
    "fig.set_figwidth(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "As we increase the degree of the polynomial model we should expect the curve to fit better the data. However, higher level polynomial models will not be as accurate when predicting the value of the dependent variable for new unobserved observations, specially outside the range of values of the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
