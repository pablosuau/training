{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I apply k-nearest neighbours classification (KNN) to a toy dataset. KNN is a non-parametric method that works well with data in which the classification boundary is irregular. To that effect, I will first create such type of dataset by randomly splitting the 2D plane by means of an irregular classification border and then sampling some training data. \n",
    "\n",
    "## Create the toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use gaussian mixtures to split the 2D space into areas in which each of the two classes in the toy dataset have a higher likelihood. Each class is defined by a mixture of 5 2D Gaussian distributions with random means and identity covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_MEANS = 10\n",
    "NUMBER_SAMPLES = 200\n",
    "\n",
    "means_0 = (np.random.random_sample(2 * NUMBER_MEANS) * 10).reshape(NUMBER_MEANS, 2)\n",
    "means_1 = (np.random.random_sample(2 * NUMBER_MEANS) * 10).reshape(NUMBER_MEANS, 2)\n",
    "\n",
    "def pdf_2d_norm(y, x, mean):\n",
    "    dif = np.array([y, x]) - mean\n",
    "    product = np.dot(np.dot(dif.T, np.linalg.inv(np.identity(2))), dif)\n",
    "    return (2 * math.pi) ** (-1) * np.linalg.det(np.identity(2)) ** (-0.5) * math.exp(-0.5 * product)\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "labels = []\n",
    "for s in range(NUMBER_SAMPLES):\n",
    "    ys.append((np.random.random_sample(1) * 10)[0])\n",
    "    xs.append((np.random.random_sample(1) * 10)[0])\n",
    "    value_0 = 0\n",
    "    value_1 = 0\n",
    "    for i in range(NUMBER_MEANS):\n",
    "        value_0 = value_0 + pdf_2d_norm(ys[-1], xs[-1], means_0[i, :])\n",
    "        value_1 = value_1 + pdf_2d_norm(ys[-1], xs[-1], means_1[i, :])\n",
    "    labels.append(int(value_1 > value_0))\n",
    "\n",
    "data = np.vstack((ys, xs)).T\n",
    "labels = np.array(labels)\n",
    "    \n",
    "index_0 = np.where(labels == 0)[0]\n",
    "index_1 = np.where(labels == 1)[0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data[index_0, 0], data[index_0, 1])\n",
    "ax.scatter(data[index_1, 0], data[index_1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "KNN is not a generalisation method. In fact, it does not even require training a model. We just need to store the training data. To classify a new observation, we just detect its k-nearest trainining observations and assign the class by means of a voting mechanism. \n",
    "\n",
    "The usual solution is an uniformly weighted one in which all the nearest observations' votes have the same value.\n",
    "\n",
    "In this example we classify a single observation using the 5 nearest neighbours. I am using squared euclidean distance to calcualte the distance between the test observation and the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(observation, data, labels, k):\n",
    "    distances = ((data - observation) ** 2).sum(axis = 1)\n",
    "    votes = labels[np.argsort(distances)][:k]\n",
    "\n",
    "    if votes.sum() == k / 2:\n",
    "        # return a random label if there is a tie\n",
    "        return round(np.random.rand())\n",
    "    else:\n",
    "        return int(votes.sum() > (k / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVATION = np.array([7, 2])\n",
    "K = 5\n",
    "label = knn(OBSERVATION, data, labels, K)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data[index_0, 0], data[index_0, 1])\n",
    "ax.scatter(data[index_1, 0], data[index_1, 1])\n",
    "ax.scatter(OBSERVATION[0], OBSERVATION[1])\n",
    "ax.legend(['class 0', 'class 1', 'test'], loc = 'center left', bbox_to_anchor = (1, 0.5))\n",
    "_ = ax.set_title('test observation assigned to class ' + str(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of k is strongly depending on the data. As we increase the value of k we mitigate more the effect of noise, but the classification borders are less distinct.\n",
    "\n",
    "In this example we observe the effect of increasing the value of k in classification results. Each coloured area indicates to which class would an observation in that area be assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colormap = plt.cm.RdYlBu\n",
    "\n",
    "for k in [1, 5, 9]:\n",
    "    fig, ax = plt.subplots()\n",
    "    xx = np.linspace(0, 10, 1000)\n",
    "    res = np.zeros((xx.shape[0], xx.shape[0]))\n",
    "    for i in range(xx.shape[0]):\n",
    "        for j in range(xx.shape[0]):\n",
    "            res[j, i] = knn(np.array([xx[i], xx[j]]), data, labels, k)\n",
    "\n",
    "    ext = [0, 10, 0, 10]\n",
    "    plt.imshow(res, zorder = 0, extent = ext, alpha = 0.5, cmap = colormap, origin = 'lower')\n",
    "    ax.scatter(data[index_0, 0], data[index_0, 1], c = colormap(0))\n",
    "    ax.scatter(data[index_1, 0], data[index_1, 1], c = colormap(256))\n",
    "\n",
    "    aspect = res.shape[0] / float(res.shape[1]) * ((ext[1] - ext[0]) / (ext[3] - ext[2]))\n",
    "    plt.gca().set_aspect(aspect)\n",
    "    fig.set_figwidth(8)\n",
    "    fig.set_figheight(8)\n",
    "    ax.set_title('k = ' + str(k))\n",
    "    \n",
    "# TODO: refactor and put in a function to use it later\n",
    "# TODO: plots in a grid so I can more easily compare (smaller?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weighted approach in which the weight of each neighbour's vote for the classification of the test observation depends on the inverse of the distance from the test observation to the test one is recommended in those cases in which the data is not uniformly sampled. Let's see some classification results as we increase the value of k in the case of the weighted approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "In this section we apply model selection to detect the best value of k. In order to do so we apply 10-fold cross validation on the training data. We are considering the uniformly-weighted case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
