This directory contains the code for my submissions to the Titanic Kaggle competition. I use this competition as a way of practicing machine learning algorithms with a dataset that is a step further from toy datasets. My objective is not to get the best result or win the competition, but to see how different algorithms compare in terms of accuracy for this particular classification problem. 

| Algorithm           | Extra                                | Accuracy | Notebook |
|---------------------|--------------------------------------|----------|----------|
| Logistic Regression | Forward selection                    | 75.12%   | [02_logistic_regression](02_logistic_regression.ipynb) |
| Logistic Regression | Forward selection and regularisation | 75.12%   | [02_logistic_regression](02_logistic_regression.ipynb) |
| 1NN Mahalanobis     | Forward selection                    | 74.16%   | [03_mahalanobis](03_mahalanobis.ipynb) |
| Logistic Regression | Backward elimination                 | 73.21%   | [02_logistic_regression](02_logistic_regression.ipynb) |
| Logistic Regression |                                      | 72.72%   | [02_logistic_regression](02_logistic_regression.ipynb) |
| 1NN Mahalanobis     |                                      | 64.59%   | [03_mahalanobis](03_mahalanobis.ipynb) |
| 1NN Mahalanobis     | Backward elimination                 | 61.72%   | [03_mahalanobis](03_mahalanobis.ipynb) |