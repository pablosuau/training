{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = pd.read_csv('../../datasets/titanic_training_processed.csv')\n",
    "df_test = pd.read_csv('../../datasets/titanic_test_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just an experiment to practice with the concept of statistical distances. The idea is to visualise a dimensionality reduced representation of the two classes (survived / non-survived) to get some awarenenss of their separability and then use a simple Mahalanobis distance between each test observation and the two classes to perform classification. \n",
    "\n",
    "Originally I wanted to apply t-SNE for dimensionality reduction, but the t-SNE algorithm in scikit-learn cannot be applied to new data. Therefore, I am sticking with PCA instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_training.values[:, 2:]\n",
    "pca = PCA(n_components = 2)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_1 = df_training.Survived == 1\n",
    "index_0 = df_training.Survived == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "_ = ax.scatter(X_pca[index_1, 0], X_pca[index_1, 1], c = 'blue', alpha = 0.5)\n",
    "_ = ax.scatter(X_pca[index_0, 0], X_pca[index_0, 1], c = 'red', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a large overlap between the two distributions. What is the Bhattacharyya distance value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_observations(array, x, y):\n",
    "    return np.sum(np.logical_and(array[:, 0] < x, \n",
    "                                 np.logical_and(array[:, 0] >= x - 0.1, \n",
    "                                                np.logical_and(array[:, 1] < y, \n",
    "                                                               array[:, 1] >= y - 0.1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discrete = np.arange(-4, 10, 0.1)\n",
    "n_1 = np.sum(index_1)\n",
    "n_0 = np.sum(index_0)\n",
    "bc = 0\n",
    "for i in list(itertools.product(discrete, discrete)):\n",
    "    x = i[0]\n",
    "    y = i[1]\n",
    "    p_1 = number_observations(X_pca[index_1, :], x, y) / n_1\n",
    "    p_0 = number_observations(X_pca[index_0, :], x, y) / n_0\n",
    "    bc = bc + math.sqrt(p_1 * p_0)\n",
    "print('Bhattacharyya distance: ' + str(-math.log(bc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now using Mahalanobis distance to assign a class to each test observation. In order to do that, we need first to apply the same transforation that we applied to the traning data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca = pca.transform(df_test.values[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "_ = ax.scatter(X_pca[index_1, 0],  X_pca[index_1, 1], c = 'blue', alpha = 0.5)\n",
    "_ = ax.scatter(X_pca[index_0, 0], X_pca[index_0, 1], c = 'red', alpha = 0.5)\n",
    "_ = ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c = 'gray', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And additionally we need to estimate the parameters of the two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_0 = np.mean(X_pca[index_0, :], axis = 0)\n",
    "mean_1 = np.mean(X_pca[index_1, :], axis = 0)\n",
    "cov_0 = np.cov(X_pca[index_0, :].T)\n",
    "cov_1 = np.cov(X_pca[index_1, :].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis(p, mean, cov):\n",
    "    dif = p - mean\n",
    "    return math.sqrt(np.dot(np.dot(dif.T, np.linalg.inv(cov)), dif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "for i in range(X_test_pca.shape[0]):\n",
    "    dist_0 = mahalanobis(X_test_pca[i, :], mean_0, cov_0)\n",
    "    dist_1 = mahalanobis(X_test_pca[i, :], mean_1, cov_1)\n",
    "    y_test.append(int(dist_1 < dist_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = df_test.copy()\n",
    "submission['Survived'] = y_test\n",
    "submission = submission[['PassengerId', 'Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submissions/03_mahalanobis.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission achieved a 64.59% accuracy (much lower than the accuracy I got with logistic regression.) This method is not valid due to two factors: there is a large overlap between the two classes, and the distributions do not look like normal distributions. A similar but simpler approach, KNN, probably would have obtained a higher accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "It is unlikely that we will improve the results much. However, just for the sake of the experiment, we are trying a feature selection approach (both forward selection and backward elimination). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating sets for 10-fold cross validation\n",
    "indexes = list(range(len(df_training)))\n",
    "random.shuffle(indexes)\n",
    "folds = []\n",
    "for i in range(10):\n",
    "    folds.append([])\n",
    "for i in range(len(indexes)):\n",
    "    folds[i % 10].append(indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_training_test_set(df_training, train_indexes, test_indexes, column_indexes):\n",
    "    columns = df_training.columns[column_indexes]\n",
    "    datasets = {}\n",
    "    datasets['X_train'] = df_training.iloc[train_indexes][columns].values\n",
    "    datasets['X_test'] = df_training.iloc[test_indexes][columns].values\n",
    "    datasets['y_train'] = df_training.iloc[train_indexes]['Survived'].values\n",
    "    datasets['y_test'] = df_training.iloc[test_indexes]['Survived'].values\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(datasets):\n",
    "    if datasets['X_train'].shape[1] > 2:\n",
    "        pca = PCA(n_components = 2)\n",
    "        X_training_pca = pca.fit_transform(datasets['X_train'])\n",
    "        X_test_pca = pca.transform(datasets['X_test'])\n",
    "    else:\n",
    "        X_training_pca = datasets['X_train']\n",
    "        X_test_pca = datasets['X_test']\n",
    "        \n",
    "    mean_0 = np.mean(X_training_pca[datasets['y_train'] == 0, :], axis = 0)\n",
    "    mean_1 = np.mean(X_training_pca[datasets['y_train'] == 1, :], axis = 0)\n",
    "    cov_0 = np.cov(X_training_pca[datasets['y_train'] == 0, :].T)\n",
    "    cov_1 = np.cov(X_training_pca[datasets['y_train'] == 1, :].T)\n",
    "    \n",
    "    y_pred = []\n",
    "    for i in range(X_test_pca.shape[0]):\n",
    "        if datasets['X_train'].shape[1] > 1:\n",
    "            dist_0 = mahalanobis(X_test_pca[i, :], mean_0, cov_0)\n",
    "            dist_1 = mahalanobis(X_test_pca[i, :], mean_1, cov_1)\n",
    "        else:\n",
    "            dist_0 = (X_test_pca[i, :] - mean_0) / cov_0\n",
    "            dist_1 = (X_test_pca[i, :] - mean_1) / cov_1\n",
    "        y_pred.append(int(dist_1 < dist_0))\n",
    "        \n",
    "    return sqrt(np.sum(np.power(np.array(y_pred) - np.array(datasets['y_test']), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(df_training, folds, column_indexes):\n",
    "    error = 0\n",
    "    \n",
    "    for k in range(10):\n",
    "        train_indexes = []\n",
    "        for j in range(10):\n",
    "            if j == k:\n",
    "                test_indexes = folds[j]\n",
    "            else:\n",
    "                train_indexes = train_indexes + folds[j]\n",
    "                \n",
    "        datasets = produce_training_test_set(df_training, train_indexes, test_indexes, column_indexes)\n",
    "        \n",
    "        error = error + evaluate(datasets)\n",
    "        \n",
    "    return error / 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indexes = list(range(2, 62))\n",
    "k_fold_cross_validation(df_training, folds, column_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward selection\n",
    "pending = list(range(2, 62))\n",
    "model = []\n",
    "min_error = sys.float_info.max\n",
    "while len(pending) > 0:\n",
    "    \n",
    "    prev_error = min_error\n",
    "    min_error = sys.float_info.max\n",
    "    \n",
    "    for i in pending:\n",
    "        new_model = model + [i]\n",
    "        error = k_fold_cross_validation(df_training, folds, new_model)\n",
    "        \n",
    "        if error < min_error:\n",
    "            min_error = error\n",
    "            best_model = new_model\n",
    "            feature = i\n",
    "            \n",
    "    if min_error < prev_error:\n",
    "        print('Selecting feature ' + df_training.columns[feature] + ' - error decreased to ' + str(min_error))\n",
    "        model = best_model\n",
    "        pending.remove(feature)\n",
    "    else:\n",
    "        print('END')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "X_training_pca = pca.fit_transform(df_training.values[:, model])\n",
    "X_test_pca = pca.transform(df_test.values[:, model])\n",
    "\n",
    "mean_0 = np.mean(X_training_pca[index_0, :], axis = 0)\n",
    "mean_1 = np.mean(X_training_pca[index_1, :], axis = 0)\n",
    "cov_0 = np.cov(X_training_pca[index_0, :].T)\n",
    "cov_1 = np.cov(X_training_pca[index_1, :].T)\n",
    "\n",
    "y_pred = []\n",
    "for i in range(X_test_pca.shape[0]):\n",
    "    dist_0 = mahalanobis(X_test_pca[i, :], mean_0, cov_0)\n",
    "    dist_1 = mahalanobis(X_test_pca[i, :], mean_1, cov_1)\n",
    "    y_pred.append(int(dist_1 < dist_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = df_test.copy()\n",
    "submission['Survived'] = y_pred\n",
    "submission = submission[['PassengerId', 'Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submissions/03_mahalanobis_forward_selection.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly enough, feature selection actually produced a much worse result: 40.69% prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
