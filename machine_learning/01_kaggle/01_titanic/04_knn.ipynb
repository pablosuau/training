{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = pd.read_csv('../../datasets/titanic_training_processed.csv')\n",
    "df_test = pd.read_csv('../../datasets/titanic_test_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_training.columns[2:]\n",
    "X_train = df_training[columns].values\n",
    "X_test = df_test[columns].values\n",
    "y_train = df_training['Survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No feature selection\n",
    "\n",
    "We have to still apply 10-fold cross validation to select weighted/no-weighted and the value of K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating sets for 10-fold cross validation\n",
    "indexes = list(range(len(df_training)))\n",
    "random.shuffle(indexes)\n",
    "folds = []\n",
    "for i in range(10):\n",
    "    folds.append([])\n",
    "for i in range(len(indexes)):\n",
    "    folds[i % 10].append(indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_training_test_set(df_training, train_indexes, test_indexes, column_indexes):\n",
    "    columns = df_training.columns[column_indexes]\n",
    "    datasets = {}\n",
    "    datasets['X_train'] = df_training.iloc[train_indexes][columns].values\n",
    "    datasets['X_test'] = df_training.iloc[test_indexes][columns].values\n",
    "    datasets['y_train'] = df_training.iloc[train_indexes]['Survived'].values\n",
    "    datasets['y_test'] = df_training.iloc[test_indexes]['Survived'].values\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(datasets, neigs, weights):\n",
    "    clf = KNeighborsClassifier(n_neighbors = neigs, weights = weights)\n",
    "    clf.fit(datasets['X_train'], datasets['y_train'])\n",
    "    y_pred = clf.predict(datasets['X_test'])\n",
    "    return sqrt(np.sum(np.power(np.array(y_pred) - np.array(datasets['y_test']), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(df_training, folds, column_indexes, neigs, weights):\n",
    "    error = 0\n",
    "    \n",
    "    for k in range(10):\n",
    "        train_indexes = []\n",
    "        for j in range(10):\n",
    "            if j == k:\n",
    "                test_indexes = folds[j]\n",
    "            else:\n",
    "                train_indexes = train_indexes + folds[j]\n",
    "                \n",
    "        datasets = produce_training_test_set(df_training, train_indexes, test_indexes, column_indexes)\n",
    "        \n",
    "        error = error + evaluate(datasets, neigs, weights)\n",
    "        \n",
    "    return error / 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(1, 200)\n",
    "W = ['uniform', 'distance']\n",
    "column_indexes = list(range(2, 62)) # All columns\n",
    "minimum = sys.float_info.max\n",
    "\n",
    "errors = dict()\n",
    "for w in W:\n",
    "    errors[w] = list()\n",
    "    for k in tqdm(K):\n",
    "        error = k_fold_cross_validation(df_training, folds, column_indexes, k, w)\n",
    "        errors[w].append(error)\n",
    "        if error < minimum:\n",
    "            minimum = error\n",
    "            min_k = k\n",
    "            min_w = w\n",
    "            \n",
    "print('Minimum for w = ' + min_w + ' and k = '+ str(min_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for w in W:\n",
    "    ax.plot(K, errors[w])\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('error')\n",
    "ax.legend(W)\n",
    "fig.set_figwidth(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = min_k, weights = min_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = df_test.copy()\n",
    "submission['Survived'] = y_test\n",
    "submission = submission[['PassengerId', 'Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./submissions/'):\n",
    "    os.makedirs('./submissions/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submissions/04_knn.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My submission to Kaggle produced a 70.81% test prediction accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection - forward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating sets for 10-fold cross validation\n",
    "indexes = list(range(len(df_training)))\n",
    "random.shuffle(indexes)\n",
    "folds = []\n",
    "for i in range(10):\n",
    "    folds.append([])\n",
    "for i in range(len(indexes)):\n",
    "    folds[i % 10].append(indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_training_test_set(df_training, train_indexes, test_indexes, column_indexes):\n",
    "    columns = df_training.columns[column_indexes]\n",
    "    datasets = {}\n",
    "    datasets['X_train'] = df_training.iloc[train_indexes][columns].values\n",
    "    datasets['X_test'] = df_training.iloc[test_indexes][columns].values\n",
    "    datasets['y_train'] = df_training.iloc[train_indexes]['Survived'].values\n",
    "    datasets['y_test'] = df_training.iloc[test_indexes]['Survived'].values\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(datasets, C = None):\n",
    "    if C is None:\n",
    "        C = 1\n",
    "    logreg = LogisticRegression(C = C)\n",
    "    logreg.fit(datasets['X_train'], datasets['y_train'])\n",
    "    y_pred = logreg.predict(datasets['X_test'])\n",
    "    return sqrt(np.sum(np.power(np.array(y_pred) - np.array(datasets['y_test']), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(df_training, folds, column_indexes, C = None):\n",
    "    error = 0\n",
    "    \n",
    "    for k in range(10):\n",
    "        train_indexes = []\n",
    "        for j in range(10):\n",
    "            if j == k:\n",
    "                test_indexes = folds[j]\n",
    "            else:\n",
    "                train_indexes = train_indexes + folds[j]\n",
    "                \n",
    "        datasets = produce_training_test_set(df_training, train_indexes, test_indexes, column_indexes)\n",
    "        \n",
    "        error = error + evaluate(datasets, C)\n",
    "        \n",
    "    return error / 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indexes = list(range(2, 62))\n",
    "k_fold_cross_validation(df_training, folds, column_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward selection\n",
    "pending = list(range(2, 62))\n",
    "model = []\n",
    "min_error = sys.float_info.max\n",
    "while len(pending) > 0:\n",
    "    \n",
    "    prev_error = min_error\n",
    "    min_error = sys.float_info.max\n",
    "    \n",
    "    for i in pending:\n",
    "        new_model = model + [i]\n",
    "        error = k_fold_cross_validation(df_training, folds, new_model)\n",
    "        \n",
    "        if error < min_error:\n",
    "            min_error = error\n",
    "            best_model = new_model\n",
    "            feature = i\n",
    "            \n",
    "    if min_error < prev_error:\n",
    "        print('Selecting feature ' + df_training.columns[feature] + ' - error decreased to ' + str(min_error))\n",
    "        model = best_model\n",
    "        pending.remove(feature)\n",
    "    else:\n",
    "        print('END')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_forward = model\n",
    "columns = df_training.columns[model_forward]\n",
    "X_train = df_training[columns].values\n",
    "X_test = df_test[columns].values\n",
    "y_train = df_training['Survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = df_test.copy()\n",
    "submission['Survived'] = y_test\n",
    "submission = submission[['PassengerId', 'Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submissions/02_logistic_regression_forward_selection.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission produced a 75.119% test prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward selection - bakward elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward elimination\n",
    "model = list(range(2, 62))\n",
    "min_error = k_fold_cross_validation(df_training, folds, column_indexes)\n",
    "while len(model) > 0:\n",
    "    \n",
    "    prev_error = min_error\n",
    "    min_error = sys.float_info.max\n",
    "    \n",
    "    for i in model:\n",
    "        new_model = model[:]\n",
    "        new_model.remove(i)\n",
    "        error = k_fold_cross_validation(df_training, folds, new_model)\n",
    "        \n",
    "        if error < min_error:\n",
    "            min_error = error\n",
    "            best_model = new_model\n",
    "            feature = i\n",
    "            \n",
    "    if min_error < prev_error:\n",
    "        print('Removing feature ' + df_training.columns[feature] + ' - error decreased to ' + str(min_error))\n",
    "        model = best_model\n",
    "    else:\n",
    "        print('END')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward eliminiation seems to reduce the prediction error for the training set even more. Let's make a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_backward = model\n",
    "columns = df_training.columns[model_backward]\n",
    "X_train = df_training[columns].values\n",
    "X_test = df_test[columns].values\n",
    "y_train = df_training['Survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = df_test.copy()\n",
    "submission['Survived'] = y_test\n",
    "submission = submission[['PassengerId', 'Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submissions/02_logistic_regression_backward_elimination.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission produced 73.205% prediction accuracy for the test set. Therefore, we should keep forward selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection - regularisation\n",
    "\n",
    "The only parameter worth playing with is the regularisation factor C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indexes = model_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.arange(0.01, 1.5, 0.01)\n",
    "rmses = []\n",
    "for c in C:\n",
    "    rmses.append(k_fold_cross_validation(df_training, folds, new_model, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(C, rmses)\n",
    "ax.set_xlabel('C')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Regularisation factor - model selection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum seems to be at 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_training.columns[model_forward]\n",
    "X_train = df_training[columns].values\n",
    "X_test = df_test[columns].values\n",
    "y_train = df_training['Survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = df_test.copy()\n",
    "submission['Survived'] = y_test\n",
    "submission = submission[['PassengerId', 'Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submissions/02_logistic_regression_regularisation.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I obtained exactly the same test accuracy after my submission than in the case of not using regularisation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
