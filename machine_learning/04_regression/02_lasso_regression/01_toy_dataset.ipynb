{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import fabs\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = 100\n",
    "DIVERGENCE_VALUE = 10\n",
    "MAX_ITERATIONS = 10000\n",
    "STOP_THRESHOLD = 0.0001\n",
    "DIMENSIONS = 7\n",
    "LAMBDA = 1\n",
    "MU = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y, coef) = make_regression(n_samples = SAMPLES, \n",
    "                               n_features = DIMENSIONS - 1, \n",
    "                               n_informative = DIMENSIONS - 1, \n",
    "                               effective_rank = 2,\n",
    "                               n_targets = 1, \n",
    "                               coef = True,\n",
    "                               bias = 3,\n",
    "                               tail_strength = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47.93845494, 60.57119573, 63.74622774, 72.78881584, 81.19385617,\n",
       "       11.56618719])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\min_{\\beta}\\frac{1}{n}||y-X\\beta||^2_{2}+\\lambda||\\beta||_1 + \\mu||\\beta||_2^2$\n",
    "\n",
    "$L=\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\left(y_i - \\left(\\beta_0 + \\displaystyle\\sum_{j=1}^{6} \\beta_j x_{ij}\\right)\\right)^2 + \\lambda\\displaystyle\\sum_{j=0}^{6}|\\beta_j| + \\mu\\displaystyle\\sum_{j=0}^{6} \\beta_j^2$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\beta_0} = -\\frac{2}{n}\\displaystyle\\sum_{i=1}^{n}\\left(y_i - \\beta_0 -\\displaystyle\\sum_{j=1}^{6} \\beta_j x_{ij}\\right) + \\lambda\\frac{\\beta_0}{|\\beta_0|} + 2\\mu\\beta_0$\n",
    "\n",
    "And for $\\beta_{k\\neq 0}$:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\beta_{k}} = -\\frac{2}{n}\\displaystyle\\sum_{i=1}^{n}x_{ik}\\left(y_i - \\beta_0 -\\displaystyle\\sum_{j=1}^{6} \\beta_j x_{ij}\\right) + \\lambda\\frac{\\beta_k}{|\\beta_k|} + 2\\mu\\beta_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = 100\n",
    "K = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y, coef) = make_regression(n_samples = SAMPLES, \n",
    "                               n_features = 6, \n",
    "                               n_informative = 6, \n",
    "                               effective_rank = 2,\n",
    "                               n_targets = 1, \n",
    "                               coef = True,\n",
    "                               bias = 3,\n",
    "                               tail_strength = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((SAMPLES, 1)), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function for backtracking line search\n",
    "def line_search(current_params, gradient, beta):\n",
    "    current_params = np.array(current_params)\n",
    "    \n",
    "    def _loss(params):\n",
    "        y_pred = np.matmul(X, np.array(current_params))\n",
    "        return np.mean(np.power(y - y_pred, 2), axis = 0)\n",
    "        \n",
    "    t = 1.0\n",
    "    while _loss(current_params - t * gradient) > _loss(current_params) - \\\n",
    "                                                 t/2.0 * norm(gradient, ord = 2) ** 2:\n",
    "        t = t * beta\n",
    "        \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function to calculate gradient\n",
    "# l = lambda\n",
    "# m = mu\n",
    "def calculate_gradient(x, y, l, m, current_params):\n",
    "    db = np.zeros(7)    \n",
    "    \n",
    "    # Common term\n",
    "    common = (y - \n",
    "              current_params[0] - \n",
    "              np.sum(np.multiply(np.tile(current_params[1:], (SAMPLES, 1)), x[:, 1:]), axis=1))\n",
    "    \n",
    "    # Function for the regularisation factor\n",
    "    def regularisation(param, l, m):\n",
    "        return l * param / fabs(param) + 2 * m * param\n",
    "    \n",
    "    # db_0\n",
    "    db[0] = - 2 / float(SAMPLES) * np.sum(common) + regularisation(current_params[0], l, m)\n",
    "\n",
    "    # db_k, k != 0\n",
    "    for k in range(1, 7):\n",
    "        db[k] = -2 / float(SAMPLES) * np.sum(np.multiply(x[:, k], common)) + regularisation(current_params[k], l, m)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.90698589,  0.86882003,  0.87470091,  1.01822005,  1.388318  ,\n",
       "        0.79615466,  0.80103953])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_gradient(X, y, 1, 0.01, [1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shouldn't initialise to zero to prevent divisions by zero\n",
    "# when computing gradients\n",
    "current_params = [1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.000000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Coordinate gradient descent implementation\n",
    "# Cycling through the dimensions\n",
    "w = [np.array(current_params[:])]\n",
    "it = 0\n",
    "while it == 0 or (np.sum(np.abs(np.array(current_params) - np.array(w[-1]))) > STOP_THRESHOLD and \\\n",
    "                  np.all(np.array(current_params) - np.array(w[-1]) < DIVERGENCE_VALUE) and \\\n",
    "                  it < MAX_ITERATIONS):\n",
    "    w.append(current_params)\n",
    "    # Select the gradient for the next dimension\n",
    "    d = it % DIMENSIONS\n",
    "    g = np.zeros(DIMENSIONS)\n",
    "    g[d] = calculate_gradient(X, y, LAMBDA, MU, current_params)[d]\n",
    "    # Linear search in the direction of the selected dimension\n",
    "    t = line_search(current_params, g, 0.9)\n",
    "    # Update parameters\n",
    "    current_params = list(np.array(w[-1]) - t * np.array(g))[:]\n",
    "    print(current_params)\n",
    "    # Wrapping up the iteration\n",
    "    it = it + 1\n",
    "w.append(current_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
