{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was tested on a Mac. Changes may be required to run this code in a Windows machine. \n",
    "\n",
    "In these examples I am hardcoding the number of cores to use (`NUM_PROCESSES = 4`). The `multiprocessing` module will try and use all available cores, so you shouldn't need to hardcode the number of processes unless you want to manage your resources. In my experiences, at least in Windows machines, I've noticed that is better to hardcode the number of processes as number of available cores - 1. Otherwise, the operating system starts feeling laggy and unresponsive. \n",
    "\n",
    "## Estimate the value of pi using the Monte Carlo method\n",
    "\n",
    "We generate multiple random values. The proportion of random values within a unit circle (x^2 + y^2 <= 1) with respect to the total amoung of generated random values is our approximation of pi. \n",
    "\n",
    "This is an ideal first problem because the workload can be evenly split across a number of processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_nbr_points_in_quarter_circle(nbr_estimates):\n",
    "    nbr_trials_in_quarter_unit_circle = 0\n",
    "    \n",
    "    for step in range(int(nbr_estimates)):\n",
    "        x = random.uniform(0, 1)\n",
    "        y = random.uniform(0, 1)\n",
    "        is_in_unit_circle = x * x + y * y <= 1.0\n",
    "        nbr_trials_in_quarter_unit_circle += is_in_unit_circle\n",
    "    return nbr_trials_in_quarter_unit_circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses a pool of processes. The time is estimated after creating the pool, because spawning processes (as opposed to spawning threads) has some overhead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_samples_in_total = 1e8\n",
    "\n",
    "times_proc = []\n",
    "\n",
    "for num_processes in range(1,9):\n",
    "    print('Number of processes: ' + str(num_processes))\n",
    "    pool = Pool(processes = num_processes)\n",
    "    nbr_samples_per_worker = nbr_samples_in_total / num_processes\n",
    "    print('Making {} samples per worker'.format(nbr_samples_per_worker))\n",
    "    nbr_trials_per_process = [nbr_samples_per_worker] * num_processes\n",
    "\n",
    "    t1 = time.time()\n",
    "    nbr_in_unit_circles = pool.map(estimate_nbr_points_in_quarter_circle, nbr_trials_per_process)\n",
    "    # We multiply by 4 because we are producing sampels only on one quarter of the unit circle\n",
    "    pi_estimate = sum(nbr_in_unit_circles) * 4 / nbr_samples_in_total\n",
    "    print('Estimated pi ' + str(pi_estimate))\n",
    "    delta = time.time() - t1\n",
    "    print('Delta: ' + str(delta))\n",
    "    print('-----------------')\n",
    "    \n",
    "    times_proc.append(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version is based on threads. The problem with threads is that due to Python's GIL contraint (Global Interpreter Lock) only one thread can run at a time. As a consequence of this, adding more threads actually slows down the process (due to the overhead of switching between threads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_samples_in_total = 1e8\n",
    "\n",
    "times_thread = []\n",
    "\n",
    "for num_processes in range(1,9):\n",
    "    print('Number of processes: ' + str(num_processes))\n",
    "    pool = ThreadPool(processes = num_processes)\n",
    "    nbr_samples_per_worker = nbr_samples_in_total / num_processes\n",
    "    print('Making {} samples per worker'.format(nbr_samples_per_worker))\n",
    "    nbr_trials_per_process = [nbr_samples_per_worker] * num_processes\n",
    "\n",
    "    t1 = time.time()\n",
    "    nbr_in_unit_circles = pool.map(estimate_nbr_points_in_quarter_circle, nbr_trials_per_process)\n",
    "    # We multiply by 4 because we are producing sampels only on one quarter of the unit circle\n",
    "    pi_estimate = sum(nbr_in_unit_circles) * 4 / nbr_samples_in_total\n",
    "    print('Estimated pi ' + str(pi_estimate))\n",
    "    delta = time.time() - t1\n",
    "    print('Delta: ' + str(delta))\n",
    "    print('-----------------')\n",
    "    \n",
    "    times_thread.append(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(1, 9), times_proc)\n",
    "ax.plot(range(1, 9), times_thread)\n",
    "ax.set_xlabel('number of workers')\n",
    "ax.set_ylabel('time')\n",
    "ax.grid(True)\n",
    "ax.legend(['Processes', 'Threads'])\n",
    "fig.set_figwidth(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
