{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was tested on a Mac. Changes may be required to run this code in a Windows machine. \n",
    "\n",
    "In these examples I am hardcoding the number of cores to use (`NUM_PROCESSES = 4`). The `multiprocessing` module will try and use all available cores, so you shouldn't need to hardcode the number of processes unless you want to manage your resources. In my experience, at least in Windows machines, I've noticed that is better to hardcode the number of processes as number of available cores - 1. Otherwise, the operating system starts feeling laggy and unresponsive. \n",
    "\n",
    "## Estimate the value of pi using the Monte Carlo method\n",
    "\n",
    "We generate multiple random values. The proportion of random values within a unit circle (x^2 + y^2 <= 1) with respect to the total amoung of generated random values is our approximation of pi. \n",
    "\n",
    "This is an ideal first problem because the workload can be evenly split across a number of processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_nbr_points_in_quarter_circle(nbr_estimates):\n",
    "    nbr_trials_in_quarter_unit_circle = 0\n",
    "    \n",
    "    for step in range(int(nbr_estimates)):\n",
    "        x = random.uniform(0, 1)\n",
    "        y = random.uniform(0, 1)\n",
    "        is_in_unit_circle = x * x + y * y <= 1.0\n",
    "        nbr_trials_in_quarter_unit_circle += is_in_unit_circle\n",
    "    return nbr_trials_in_quarter_unit_circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses a pool of processes. The time is estimated after creating the pool, because spawning processes (as opposed to spawning threads) has some overhead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_samples_in_total = 1e8\n",
    "\n",
    "times_proc = []\n",
    "\n",
    "for num_processes in range(1,9):\n",
    "    print('Number of processes: ' + str(num_processes))\n",
    "    pool = Pool(processes = num_processes)\n",
    "    nbr_samples_per_worker = nbr_samples_in_total / num_processes\n",
    "    print('Making {} samples per worker'.format(nbr_samples_per_worker))\n",
    "    nbr_trials_per_process = [nbr_samples_per_worker] * num_processes\n",
    "\n",
    "    t1 = time.time()\n",
    "    nbr_in_unit_circles = pool.map(estimate_nbr_points_in_quarter_circle, nbr_trials_per_process)\n",
    "    # We multiply by 4 because we are producing sampels only on one quarter of the unit circle\n",
    "    pi_estimate = sum(nbr_in_unit_circles) * 4 / nbr_samples_in_total\n",
    "    print('Estimated pi ' + str(pi_estimate))\n",
    "    delta = time.time() - t1\n",
    "    print('Delta: ' + str(delta))\n",
    "    print('-----------------')\n",
    "    \n",
    "    pool.close()\n",
    "    \n",
    "    times_proc.append(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version is based on threads. The problem with threads is that due to Python's GIL contraint (Global Interpreter Lock) only one thread can run at a time. As a consequence of this, adding more threads actually slows down the process (due to the overhead of switching between threads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_samples_in_total = 1e8\n",
    "\n",
    "times_thread = []\n",
    "\n",
    "for num_processes in range(1,9):\n",
    "    print('Number of processes: ' + str(num_processes))\n",
    "    pool = ThreadPool(processes = num_processes)\n",
    "    nbr_samples_per_worker = nbr_samples_in_total / num_processes\n",
    "    print('Making {} samples per worker'.format(nbr_samples_per_worker))\n",
    "    nbr_trials_per_process = [nbr_samples_per_worker] * num_processes\n",
    "\n",
    "    t1 = time.time()\n",
    "    nbr_in_unit_circles = pool.map(estimate_nbr_points_in_quarter_circle, nbr_trials_per_process)\n",
    "    # We multiply by 4 because we are producing sampels only on one quarter of the unit circle\n",
    "    pi_estimate = sum(nbr_in_unit_circles) * 4 / nbr_samples_in_total\n",
    "    print('Estimated pi ' + str(pi_estimate))\n",
    "    delta = time.time() - t1\n",
    "    print('Delta: ' + str(delta))\n",
    "    print('-----------------')\n",
    "    \n",
    "    pool.close()\n",
    "    \n",
    "    times_thread.append(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(1, 9), times_proc)\n",
    "ax.plot(range(1, 9), times_thread)\n",
    "ax.set_xlabel('number of workers')\n",
    "ax.set_ylabel('time')\n",
    "ax.grid(True)\n",
    "ax.legend(['Processes', 'Threads'])\n",
    "fig.set_figwidth(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the value of pi using the Monte Carlo method (numpy version)\n",
    "\n",
    "In this version we need to explicitely set numpy's random seed for each forked process (not for threads). Otherwise, all the workers wil generate exactly the same sequence of random numbers. This is because all forks share the same shared state. This is not the case with the `random` module, which was used above, because this is dealt with by the `multiprocessing` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_nbr_points_in_quarter_circle(nbr_samples):\n",
    "    np.random.seed()\n",
    "    \n",
    "    xs = np.random.uniform(0, 1, nbr_samples)\n",
    "    ys = np.random.uniform(0, 1, nbr_samples)\n",
    "    estimate_inside_quarter_unit_circle = (xs * xs + ys * ys) <= 1\n",
    "    nbr_trials_in_quarter_unit_circle = np.sum(estimate_inside_quarter_unit_circle)\n",
    "    return nbr_trials_in_quarter_unit_circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_samples_in_total = 1e8\n",
    "\n",
    "times_proc = []\n",
    "\n",
    "for num_processes in range(1,9):\n",
    "    print('Number of processes: ' + str(num_processes))\n",
    "    pool = Pool(processes = num_processes)\n",
    "    nbr_samples_per_worker = int(nbr_samples_in_total / num_processes)\n",
    "    print('Making {} samples per worker'.format(nbr_samples_per_worker))\n",
    "    nbr_trials_per_process = [nbr_samples_per_worker] * num_processes\n",
    "\n",
    "    t1 = time.time()\n",
    "    nbr_in_unit_circles = pool.map(estimate_nbr_points_in_quarter_circle, nbr_trials_per_process)\n",
    "    # We multiply by 4 because we are producing sampels only on one quarter of the unit circle\n",
    "    pi_estimate = sum(nbr_in_unit_circles) * 4 / nbr_samples_in_total\n",
    "    print('Estimated pi ' + str(pi_estimate))\n",
    "    delta = time.time() - t1\n",
    "    print('Delta: ' + str(delta))\n",
    "    print('-----------------')\n",
    "    \n",
    "    pool.close()\n",
    "    \n",
    "    times_proc.append(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_samples_in_total = 1e8\n",
    "\n",
    "times_thread = []\n",
    "\n",
    "for num_processes in range(1,9):\n",
    "    print('Number of processes: ' + str(num_processes))\n",
    "    pool = ThreadPool(processes = num_processes)\n",
    "    nbr_samples_per_worker = int(nbr_samples_in_total / num_processes)\n",
    "    print('Making {} samples per worker'.format(nbr_samples_per_worker))\n",
    "    nbr_trials_per_process = [nbr_samples_per_worker] * num_processes\n",
    "\n",
    "    t1 = time.time()\n",
    "    nbr_in_unit_circles = pool.map(estimate_nbr_points_in_quarter_circle, nbr_trials_per_process)\n",
    "    # We multiply by 4 because we are producing sampels only on one quarter of the unit circle\n",
    "    pi_estimate = sum(nbr_in_unit_circles) * 4 / nbr_samples_in_total\n",
    "    print('Estimated pi ' + str(pi_estimate))\n",
    "    delta = time.time() - t1\n",
    "    print('Delta: ' + str(delta))\n",
    "    print('-----------------')\n",
    "    \n",
    "    pool.close()\n",
    "    \n",
    "    times_thread.append(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(1, 9), times_proc)\n",
    "ax.plot(range(1, 9), times_thread)\n",
    "ax.set_xlabel('number of workers')\n",
    "ax.set_ylabel('time')\n",
    "ax.grid(True)\n",
    "ax.legend(['Processes', 'Threads'])\n",
    "fig.set_figwidth(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this vectorised version of the code is much faster than the first version on this notebook based on pure Python. \n",
    "\n",
    "Adding more threads decreases the overall execution time. This is because numpy can achieve additional spped ups by operating outside the GIL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding prime numbers\n",
    "\n",
    "We test for prime numbers over a large number range. This is a different problem to pi value estimation because the workload depends on the location in the number range and each number's check has an unpredictable complexity. However, the problem is still embarrasingly parallel (we do not need to pass state information between processes).\n",
    "\n",
    "The `multiprocessing` module, by default, divides the total number of items to compute by the number of processors. This may be the best approach if the computation of each item takes approximately the same amount of time. \n",
    "\n",
    "Strategies for efficiently use `multiprocessing` for embarrassingly parallel problems:\n",
    "\n",
    "- Split your jobs into independent units of work\n",
    "- If your workers take varying amounts of time, then consider randomizing the sequence of work\n",
    "- Sorting your work queue so slowest jobs go first may help\n",
    "- Use the default chunksize unless you have verified reasons to change it\n",
    "- Align the number of jobs with the number of physical cpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function that we use to check whether a single number is primer or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_prime(n):\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "    from_i = 3\n",
    "    to_i = math.sqrt(n) + 1\n",
    "    for i in range(from_i, int(to_i), 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use this function to check a list of numbers (the `numbers` parameter) using a pool of workers with a given `chunksize`, that is, how many numbers are assigned to each worker. This function returns the total time it took to perform the computation (the time used to create the pool of workers is not taken into account)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_primes(chunksize, numbers): \n",
    "    num_processes = 4\n",
    "\n",
    "    pool = Pool(processes = num_processes)\n",
    "    t1 = time.time()\n",
    "    is_prime = pool.map(check_prime, numbers, chunksize)\n",
    "    delta = time.time() - t1\n",
    "\n",
    "    pool.close()   \n",
    "    \n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numbers():\n",
    "    for n in range(100000000, 101000000):\n",
    "        yield n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_primes(100, generate_numbers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to try and compare some of the strategies enumerated above and compare execution times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline - just increasing the number of chunks, no considering the number of processes\n",
    "numbers = list(generate_numbers())\n",
    "chunksizes_b = [int(len(numbers)/chunks) for chunks in range(1, 16)]\n",
    "times_baseline = []\n",
    "for chunksize in tqdm(chunksizes_b):\n",
    "    times_baseline.append(check_primes(chunksize, generate_numbers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using chunksizes that are multiplo of the number of processes\n",
    "chunksizes = []\n",
    "i = 0\n",
    "while 2 ** i < 10000:\n",
    "    chunksizes.append(2 ** i)\n",
    "    i = i + 1\n",
    "times_multiplo = []\n",
    "for chunksize in tqdm(chunksizes):\n",
    "    times_multiplo.append(check_primes(chunksize, generate_numbers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomising the jobs' order, hoping that that way we will balance the load\n",
    "random.seed(0)\n",
    "random.shuffle(numbers)\n",
    "times_random = []\n",
    "for chunksize in tqdm(chunksizes):\n",
    "    times_random.append(check_primes(chunksize, numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse the jobs order, hopping that biggest jobs will be at the beginning\n",
    "numbers = list(generate_numbers())[::-1]\n",
    "times_reverse = []\n",
    "for chunksize in tqdm(chunksizes):\n",
    "    times_reverse.append(check_primes(chunksize, numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(chunksizes, times_multiplo)\n",
    "#plt.plot(chunksizes_b, times_baseline)\n",
    "ax.plot(chunksizes, times_random)\n",
    "ax.plot(chunksizes, times_reverse)\n",
    "#ax.set_yscale('log')\n",
    "ax.grid(True)\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksizes_b = [siz for siz in range(30, 8000, 30)]\n",
    "chunksizes_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(numbers)/8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: the plot is not correct: it should have number of chunks instead of chunksize on the x axis. Besides,\n",
    "# how is it possible that as I increase the chunksize the time decreases? It seems like if the chunksize \n",
    "# parameter in pool's map function is actually the number of chunks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
